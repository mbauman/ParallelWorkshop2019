{
 "cells": [
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Pkg; Pkg.activate(@__DIR__); Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# GPUs\n",
    "\n",
    "The graphics processor in your computer is _itself_ like a mini-computer highly\n",
    "tailored for massively and embarassingly parallel operations (like computing how light will bounce\n",
    "off of every point on a 3D mesh of triangles).\n",
    "\n",
    "Of course, recently their utility in other applications has become more clear\n",
    "and thus the GPGPU was born.\n",
    "\n",
    "Just like how we needed to send data to other processes, we need to send our\n",
    "data to the GPU to do computations there."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## How is a GPU different from a CPU?\n",
    "\n",
    "This is what a typical consumer CPU looks like:\n",
    "\n",
    "![](https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Parallel_Computing/images/i7.jpg)\n",
    "\n",
    "And this is what a GPU looks like:\n",
    "\n",
    "![](https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Parallel_Computing/images/GK110.jpg)\n",
    "\n",
    "Each SMX isn't just one \"core\", each is a _streaming multiprocessor_ capable of running hundreds of threads simultaneously itself.  There are so many threads, in fact, that you reason about them in groups of 32 — called a \"warp.\"  No, no [that warp](https://www.google.com/search?tbm=isch&q=warp&tbs=imgo:1), [this one](https://www.google.com/search?tbm=isch&q=warp%20weaving&tbs=imgo:1).\n",
    "\n",
    "The card above supports up to 6 warps per multiprocessor, with 32 threads each, times 15 multiprocessors... 2880 threads at a time!\n",
    "\n",
    "Also note the memory interfaces.\n",
    "\n",
    "--------------\n",
    "\n",
    "Each thread is relatively limited — and a warp is almost like a SIMD unit that supports branching. Except it's still only executing one instruction even after a branch:\n",
    "\n",
    "![](https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Parallel_Computing/images/warp-branch.png)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "You can inspect the installed GPUs with nvidia-smi:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "run(`nvidia-smi`)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Example\n",
    "\n",
    "The deep learning MNIST example: https://fluxml.ai/experiments/mnist/\n",
    "\n",
    "This is how it looks on the CPU:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "\n",
    "imgs = MNIST.images()\n",
    "labels = onehotbatch(MNIST.labels(), 0:9)\n",
    "\n",
    "# Partition into batches of size 32\n",
    "train = [(cat(float.(imgs[i])..., dims = 4), labels[:,i])\n",
    "         for i in partition(1:60_000, 32)]\n",
    "# Prepare test set (first 1,000 images)\n",
    "tX = cat(float.(MNIST.images(:test)[1:1000])..., dims = 4)\n",
    "tY = onehotbatch(MNIST.labels(:test)[1:1000], 0:9)\n",
    "\n",
    "m = Chain(\n",
    "        Conv((3, 3), 1=>32, relu),\n",
    "        Conv((3, 3), 32=>32, relu),\n",
    "        x -> maxpool(x, (2,2)),\n",
    "        Conv((3, 3), 32=>16, relu),\n",
    "        x -> maxpool(x, (2,2)),\n",
    "        Conv((3, 3), 16=>10, relu),\n",
    "        x -> reshape(x, :, size(x, 4)),\n",
    "        Dense(90, 10), softmax)\n",
    "\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))\n",
    "opt = ADAM()\n",
    "Flux.train!(loss, train[1:1], opt, cb = () -> @show(accuracy(tX, tY)))\n",
    "@time Flux.train!(loss, Flux.params(m), train[1:10], opt, cb = () -> @show(accuracy(tX, tY)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now let's re-do it on a GPU. \"All\" it takes is moving the data there with `gpu`!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "include(datapath(\"scripts/fixupCUDNN.jl\")) # JuliaBox uses an old version of CuArrays; this backports a fix for it\n",
    "gputrain = gpu.(train[1:10])\n",
    "gpum = gpu(m)\n",
    "gputX = gpu(tX)\n",
    "gputY = gpu(tY)\n",
    "gpuloss(x, y) = crossentropy(gpum(x), y)\n",
    "gpuaccuracy(x, y) = mean(onecold(gpum(x)) .== onecold(y))\n",
    "gpuopt = ADAM()\n",
    "Flux.train!(gpuloss, Flux.params(gpum), gpu.(train[1:1]), gpuopt, cb = () -> @show(gpuaccuracy(gputX, gputY)))\n",
    "@time Flux.train!(gpuloss, Flux.params(gpum), gputrain, gpuopt, cb = () -> @show(gpuaccuracy(gputX, gputY)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Defining your own GPU kernels\n",
    "\n",
    "So that's leveraging Flux's ability to work with GPU arrays — which is magical\n",
    "and awesome — but you don't always have a library to lean on like that.\n",
    "How might you define your own GPU kernel?\n",
    "\n",
    "Recall the monte carlo pi example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function serialpi(n)\n",
    "    inside = 0\n",
    "    for i in 1:n\n",
    "        x, y = rand(), rand()\n",
    "        inside += (x^2 + y^2 <= 1)\n",
    "    end\n",
    "    return 4 * inside / n\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "How could we express this on the GPU?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using CuArrays.CURAND\n",
    "function findpi_gpu(n)\n",
    "    4 * sum(curand(Float64, n).^2 .+ curand(Float64, n).^2 .<= 1) / n\n",
    "end\n",
    "findpi_gpu(10_000_000)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using BenchmarkTools\n",
    "@btime findpi_gpu(10_000_000)\n",
    "@btime serialpi(10_000_000)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "That leans on broadcast to build the GPU kernel — and is creating three arrays\n",
    "in the process — but it's still much faster than our serial pi from before."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In general, using CuArrays and broadcast is one of the best ways to just\n",
    "get everything to work. If you really want to get your hands dirty, you\n",
    "can use [CUDAnative.jl](https://github.com/JuliaGPU/CUDAnative.jl) to manually specify exactly how everything works,\n",
    "but be forewarned, it's not for the [faint at heart](https://github.com/JuliaGPU/CUDAnative.jl/blob/master/examples/reduce/reduce.jl)! (If you've done CUDA\n",
    "programming in C or C++, it's very similar.)"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.2-pre.0"
  },
  "kernelspec": {
   "name": "julia-1.1",
   "display_name": "Julia 1.1.2-pre.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
